= Chapter 3: Microservice Extraction: Focus On Growth
:toc:

== Case

=== Overview

After separating the modules per project in our modular monolith, we were able to handle different areas well:

- Synchronisation conflicts - after splitting into multiple projects, there were fewer conflicts when committing to the main branch.
- Changes were encapsulated in module projects.
- We applied patterns that fit the needs.

However, we have now reached the limit of our capacity. New issues have arisen, and we can see that we are on the edge of resource usage. In addition, the cost of scaling the entire application had increased significantly, we have more development teams and security levels are different between modules. Another point is the risk of deployment of an entire single unit. We might break something - even when there is a gret automation of everything and we follow continuous deployments, problems can still arise.

It makes sense to analyse our modular monolith and extract the relevant parts to microservices. 

NOTE: This step makes your solution much more complex than before. You have to think about network failures, latency, secure communications and many other things that are often forgotten. Think twice before you decide to do this. Look at our main assumptions - they will help you to understand the reasoning (disintegrator) for a module extraction.

IMPORTANT: To keep the code simple and understandable while comparing it to the second chapter, we have not added any new features (business processes).

=== Requirements

As mentioned in the overview, the requirements remain unchanged to keep the codebase comparable to the previous step. We will continue to do this throughout the chapters.

=== Main assumptions

Due to changing requirements and the current market situation (here you have to again imagine that this is the case, although we do not assume any new requirements), we have to adjust our assumptions:

1. Our application is now being used by 500000 people, which exceeds the maximum number of users based on our initial MVP assumptions (5000). One of our modules is used extremely often - _Contracts_ and it leads to a situation where we have to scale entire modular monolith (and it now becomes quite expensive), even though other modules usage is kept low.
2. _Contracts_ module changes quicker than other modules - the ratio is 10:1, where 10 changes are done daily to _Contracts_ and 1 change to other module like _Offers_.
3. _Contracts_ module requires higher security standards than other modules as we keep here sensitive data of our customers.
4. At the moment we have 10 development teams working on the same deployment unit (modular monolith). It leads to more and more issues related to conflicts and syncing.

Thanks to above assumptions we have 4 factors that help us to decide to extract _Contracts_ module to a microservice:

- Extreme usage
- Change frequency
- Different security level than other modules

IMPORTANT: Often there are also disintegrators related to poor communication between teams or different time zones. As you can see, these are not technical reasons. They are often related to behaviour, geographical location or other areas.

=== Solution

==== Overview

The step we take here completely changes the structure of the application. In the previous chapter - where we modularised our solution based on projects (migrating from the namespace-based modular monolith we introduced in Chapter 1) - we made significant changes to our solution, increasing the overall complexity. What happened was

- Extracting the common components (such as exception middleware or business rule validation mechanism to a separate set of projects)
- Separating each module into its own set of projects

In this way, our solution grew from 3 projects in Chapter 1 to more than 20 in Chapter 2.

In the current chapter, the complexity grows even more, as we had to

- extract contracts into a separate microservice
- extract shared components into a separate solution and create packages for each component library
- introduce a reliable message handling - RabbitMQ + MassTransit

This way our application looks like this:

image::Assets/components.jpg[]

==== Solution structure

After the changes, our solution was divided into 3 separate solutions:

1. _Fitnet.Contracts_ is a solution that contains our only microservice.
2. _Fitnet.Common_ is a solution that contains all reusable components (no common business logic, only technical components like exception middleware).
3. _Fitnet_ is a solution that contains our modular monolith with 3 modules - _Passes_, _Offers_ and _Reports_.

For each solution we have created a separate build in GH Actions. 

Components located in _Fitnet.Common_ are built into packages that are then reused by both - modular monolith and microservice. Whenever you extract another microservice, it is ready to integrate common components inside.

NOTE: As you can see, this is even more complicated than before. That's why you should always think about whether it's worth extracting one of the modules into a microservice, as it will complicate your application. Always try to find reasonable disintegrators. Otherwise, try to postpone this decision as much as possible.

==== Communication

This time we needed to move from our in-memory queue to something that could operate between 2 different instances - our modular monolith and the microservice as separate deployment units.

At this point, our application was already being used by many people, and we wanted something more reliable than before (if you remember, we suggested thinking about introducing another communication mechanism in Chapter 2).

We decided to use RabbitMQ with MassTransit. In doing so, we have created a separate component that now requires additional maintenance. Whenever an integration event arrives in the message queue, the module subscribed to it can receive it and update itself accordingly:

image::Assets/communication.jpg[]

IMPORTANT: If you are deploying a separate component like RabbitMQ for production purposes, it makes sense to create at least 1 other replica (our suggestion is to have 2) - in case the first instance goes down, another instance can handle the message communication. This way you protect your application from random failures (or rather, you reduce the chance of your application going down :)).

==== Tests

Our test structure has not changed since Chapter 2. The only difference is that they have been moved to a separate solution along with _Fitnet.Common_ and _Fitnet.Contracts_.

The difference in content is that this time we have an additional component that we had to spin up, and that is RabbitMQ with MassTransit.

==== Miscellaneous

Shared components of _Fitnet.Common_ are published inside main repository and we are using GH Packages for it. We decided to version it with https://semver.org/[Semantic Versioning].

== How to Run?

=== Requirements
- .NET SDK
- PostgresSQL
- Docker

=== How to get .NET SDK?

To run the Fitnet application, you will need to have the recent .NET SDK installed on your computer.
Click link:https://dotnet.microsoft.com/en-us/download[here] 
to download it from the official Microsoft website.

=== How to prepare Postgres?

The Fitnet application requires PostgresSQL as a component to work properly. You can either install it directly on your system or launch it using Docker Compose.

To run PostgresSQL using Docker Compose, navigate to the `root` chapter directory using the Terminal and run the command:
1. Build the Docker compose:

[source,shell]
----
 docker-compose up
----

This will start the PostgresSQL service. Once Postgres is up and running, you can proceed to run the Fitnet application.

=== Run the Fitnet Modular Monolith Application

There are two options to run the Fitnet Modular Monolith application:

==== Option 1: Launch the Fitnet Modular Monolith application in an IDE
If you have an IDE installed, you can launch the Fitnet Modular Monolith application directly from there by following these steps:

- 1. Open the project in your IDE.
- 2. Run the project.
- 3. The Fitnet Modular Monolith application should start running.

[NOTE]
====
Fitnet supports .NET User Secrets to store local secrets like connection strings on a developer machine. We encourage you to use this feature to securely store the connection string.
====

==== Option 2: Build and run the Fitnet Modular Monolith application as a Docker container

1. Build the Docker image:

[source,shell]
----
docker build -t fitnet_modular_monolith .
----

2. Run the Docker container:

[source,shell]
----
docker run -p 8080:80 --name fitnet_modular_monolith_container fitnet_modular_monolith
----

Once the container is up and running, you should be able to access the application by navigating to http://localhost:8080 in your web browser.

=== Run the Fitness.Contracts Application

==== Option 1: Launch the Fitness.Contracts application in an IDE

To launch the Fitness.Contracts application at any IDE, follow these steps:

- 1. Open the project in your IDE.
- 2. Run the project.
- 3. The Fitness.Contracts application should start running.

==== Option 2: Build and run the Fitness.Contracts application as a Docker container

1. Build the Docker image:

[source,shell]
----
docker build -t fitness_contracts .
----

2. Run the Docker container:

[source,shell]
----
docker run -p 8081:80 --name fitness_contracts_container fitness_contracts
----

Once the container is up and running, you should be able to access the application by navigating to http://localhost:8081 in your web browser.

=== Setting up Artifactory and EvolutionaryArchitecture NuGet Packages

- 1. Open JetBrains Rider and navigate to `File > Settings > NuGet > Sources`.
- 2. Click the `+` button to add a new package source.
- 3. In the `Add Package Source` window, provide Artifactory URL in the `https://nuget.pkg.github.com/evolutionary-architecture/index.json`, fill your Github Username and PAT.
- 4. Click `OK` to confirm the new package source.
- 5. Make sure your new package source is enabled and then click `OK` to close the `Settings` window.

You should now be able to restore and download the EvolutionaryArchitecture nuget packages from your Artifactory source within Rider.

[NOTE]
====
The provided instruction is primarily intended for JetBrains Rider. However, the procedure for adding a NuGet package source in alternative IDEs like Visual Studio is quite similar.
====

=== How to run Integration Tests?
Running integration tests for both the Fitnet Modular Monolith and Fitness.Contracts applications involves similar steps, as the testing setup for both projects.
To run the integration tests for project, you can use either the command:
[source,shell]
----
dotnet test
----
or the `IDE test Explorer`. 

These tests are written using `xUnit` and require `Docker` to be running as they use `test containers` package to run PostgresSQL in a Docker container during testing. 
Therefore, make sure to have `Docker` running before executing the integration tests.
